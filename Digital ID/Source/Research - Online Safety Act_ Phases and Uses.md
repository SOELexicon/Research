

---
type: source
source-type: report
title: "The UK Online Safety Act: A Dual Mandate of Protection and Control"
author: [[Entity - Craig - Research Analyst]]
publisher: Independent Research
publication-date: 2025-09-28
url: internal-research
access-date: 2025-09-28
classification: U
reliability: A
tags: [source, source/report, uk-online-safety-act, surveillance, censorship, age-verification, encryption, research]
---

# **The UK Online Safety Act: A Dual Mandate of Protection and Control**

## **Section 1: The Architecture of the Online Safety Act**

The United Kingdom's Online Safety Act 2023 represents one of the most ambitious and contentious attempts by a major economy to regulate the digital sphere. Having received Royal Assent on 26 October 2023, the legislation establishes a comprehensive regulatory framework intended to impose a legal duty of care on online service providers to protect their users from a wide array of harms.1 This section deconstructs the fundamental architecture of the Act, outlining its stated objectives, its extensive scope, the pivotal role of its regulator, and the expansion of criminal law into online communications.

### **Stated Purpose and Legislative Journey**

The Act's publicly stated purpose is to make the UK "the safest place in the world to be online".3 Its primary goals are to protect children from harmful content and to empower adults with greater control over what they see online, while also tackling illegal activity.1 The legislation is the culmination of a protracted political process that began with the government's 2019 Online Harms White Paper, which first proposed a new regulatory framework to address the proliferation of harmful content.3 After years of debate, pre-legislative scrutiny, and significant amendments, the Act was passed into law, signifying a long-term political commitment to fundamentally reshaping the responsibilities of technology companies operating in the UK.7  
The core principle of the Act is a shift from a reactive model of content moderation, where platforms primarily respond to user reports, to a proactive one based on duties of care. This requires in-scope services to implement systems and processes designed to prevent harm from occurring in the first place, particularly concerning illegal content and material harmful to children.1

### **Scope and Extraterritorial Reach**

The legislation's scope is exceptionally broad, capturing a vast ecosystem of online services. It applies principally to two types of services: "user-to-user" services, defined as platforms that allow users to generate, upload, or share content that can be encountered by other users; and search services.6 This definition encompasses not only major social media networks, messaging applications, and video-sharing platforms but also a wide range of other services including online gaming platforms, dating apps, and even the comment sections on websites.1 While certain services like email, one-to-one voice calls, and SMS are explicitly exempt, the Secretary of State retains the power to bring them within scope in the future if they are deemed to pose a sufficient risk of harm to UK users.8  
Crucially, the Act possesses a significant extraterritorial effect. It applies not only to companies based in the UK but to any service accessible by users in the UK, regardless of where the company is located, provided it has "links with the UK".8 This concept is drawn very widely and can be triggered simply by having a significant number of UK users or if the service targets the UK market. This global reach is a deliberate design choice, intended to prevent companies from evading regulation by operating from overseas, but it also creates a complex web of international compliance challenges and potential conflicts with other legal jurisdictions.8

### **The Central Role of Ofcom**

At the heart of the new regime is the communications regulator, Ofcom, which has been granted an expansive and powerful new mandate as the independent regulator for online safety.1 Ofcom is responsible for translating the Act's broad legal duties into detailed, actionable requirements for industry. Its primary functions include developing and publishing codes of practice that set out the recommended steps services can take to comply with their safety duties, as well as issuing extensive guidance on specific obligations like risk assessments and age assurance.1  
The powers vested in Ofcom to enforce compliance are formidable and designed to command the attention of even the world's largest technology corporations. These enforcement tools include:

* **Information and Audit Powers:** Ofcom can compel companies to provide information about the measures they are taking to comply with their duties and can conduct audits of their systems and processes to assess their effectiveness.11  
* **Financial Penalties:** In cases of non-compliance, Ofcom can impose substantial fines of up to Â£18 million or 10% of a company's qualifying annual worldwide revenue, whichever amount is greater.8 This penalty structure ensures that the fines are punitive enough to serve as a credible deterrent for global tech giants.  
* **Service Restriction and Disruption:** In the most serious instances of non-compliance, Ofcom has the power to apply to the courts for an order to block a service from being accessible in the UK, a measure of last resort that underscores the gravity of the new regime.1

This framework represents a significant delegation of authority from Parliament to the regulator. While the Act sets the overarching principles, it is Ofcom that defines the practical, technical, and operational substance of compliance through its codes and guidance. These regulatory instruments can be updated more rapidly than primary legislation, allowing the regime to adapt to new technologies and emerging online harms. However, this structure also concentrates considerable power within the regulator. The government's stated intention for Ofcom to design its codes to be "as proofed against judicial review as possible" suggests a deliberate effort to insulate these detailed regulatory decisions from legal challenges, thereby solidifying Ofcom's position as a quasi-legislative body in the digital domain.4

### **New Criminal Offences**

Beyond imposing duties on service providers, the Act also expands the scope of criminal law by creating several new communication-based offences, which came into effect on 31 January 2024\.12 These include:

* **False Communications Offence:** Criminalises the sending of a message that the sender knows to be false with the intent to cause non-trivial psychological or physical harm to a likely audience.4  
* **Threatening Communications Offence:** Covers communications that convey a threat of serious harm.  
* **Intimate Image Offences:** Creates new offences related to the sharing of sexually explicit "deepfakes" and other forms of intimate image abuse.1

These new offences demonstrate the Act's dual approach: regulating the systems of platforms while also holding individuals directly accountable for specific types of harmful online behaviour.

## **Section 2: The Phased Implementation Roadmap**

The transition from legislative text to a fully operational regulatory regime is being managed through a meticulous, multi-phase implementation plan orchestrated by Ofcom. This phased approach is designed to introduce the Act's extensive duties incrementally, allowing both the regulator and the industry to adapt to the escalating compliance demands. The roadmap, which began in late 2023 and is expected to see the Act fully implemented by 2026, is structured around three core pillars: tackling illegal content, protecting children from harmful content, and imposing enhanced duties on the largest and riskiest platforms.10

### **Phase 1: Duties Regarding Illegal Content (Enforceable from March 2025\)**

The first phase of implementation focused on the foundational duty for all in-scope services to address illegal content. This phase established the principle of proactive risk management as the cornerstone of the new regime.  
The key milestone was the deadline of 16 March 2025, by which all regulated services were required to have completed their first comprehensive "illegal content risk assessment".12 This assessment mandated that platforms systematically identify, evaluate, and understand the risks of illegal content appearing on their services. The definition of illegal content is broad, encompassing material related to terrorism, child sexual exploitation and abuse (CSEA), hate speech, fraud, and the sale of illegal drugs and weapons.1  
To guide this process, Ofcom published its final illegal content codes of practice and detailed risk assessment guidance on 16 December 2024\.10 These documents provide platforms with the methodologies and recommended measures for fulfilling their duties. Following the completion of these risk assessments, the illegal content safety duties became legally enforceable by Ofcom on 17 March 2025, marking the official start of the new regulatory era.1 From this date, Ofcom could begin actively monitoring compliance and launching enforcement actions against firms failing to meet their obligations.

### **Phase 2: Child Safety and Protection from Harmful Content (Enforceable from July 2025\)**

The second and arguably most complex phase of the rollout introduced a suite of duties aimed specifically at protecting children. This phase involved a multi-step compliance journey with several critical deadlines throughout the first half of 2025\.

1. **Children's Access Assessment:** The first step required all user-to-user and search services to determine whether their platform was "likely to be accessed by children." The deadline for completing this assessment was 16 April 2025\.13 This initial evaluation was crucial as it determined which services would be subject to the Act's full child safety duties.  
2. **Children's Risk Assessment:** Services that concluded they were likely to be accessed by children were then obligated to conduct a more detailed "children's risk assessment." This involved identifying and assessing the risk of children encountering content harmful to them. The deadline for this assessment was 24 July 2025\.12 Ofcom published its guidance for this process on 24 April 2025\.17  
3. **Implementation of Safety Measures:** The culmination of this phase occurred on 25 July 2025, when the child safety duties became fully enforceable.1 From this date, relevant platforms became legally required to implement proportionate systems and processes to mitigate the risks identified. This includes protecting children from "primary priority" harmful content, such as material that encourages suicide, self-harm, or eating disorders, as well as pornography. It also covers other "priority" harms like bullying, hateful content, and content depicting serious violence or dangerous stunts.1

A central and highly visible component of Phase 2 is the mandate for "Highly Effective Age Assurance" (HEAA). This requirement signifies a major departure from previous self-declaration methods like simple tick-boxes. For providers that publish their own pornographic content, the duty to implement robust age checks meeting Ofcom's guidance came into force on 17 January 2025\.12 For user-to-user services that allow user-generated pornographic content, the HEAA requirement became effective in July 2025\.13 This has forced platforms to adopt more technologically advanced solutions, such as facial age estimation or verification through third-party data providers, to prevent minors from accessing adult material.3

### **Phase 3: Enhanced Duties for Categorised Services (Implementation from Summer 2025 into 2026\)**

The third and final phase of implementation introduces a tiered system of regulation, imposing the most stringent obligations on the largest and highest-risk services. The Act creates three tiersâCategory 1, Category 2A, and Category 2Bâwith the thresholds for each defined in secondary legislation laid before Parliament in December 2024\.10  
Ofcom was expected to publish the official register of categorised services in the summer of 2025, although this timeline has been impacted by a legal challenge from the Wikimedia Foundation regarding the regulations.7 Once categorised, these services will face a range of additional duties that will be rolled out through late 2025 and into 2026\. These enhanced obligations include:

* **Transparency Reporting:** Category 1 and 2A services will be required to publish detailed annual transparency reports outlining their risk assessments and the effectiveness of their safety measures. Ofcom is expected to begin issuing formal transparency notices to these services between August and November 2025, setting out the specific information they must report.14  
* **User Empowerment Tools:** The largest platforms (Category 1\) will have a duty to provide adult users with tools to give them greater control over the content they see, such as the ability to filter out unverified users or specific types of legal but harmful content they do not wish to encounter.5  
* **Systemic Risk Management:** Ofcom is expected to consult on further duties for categorised services between late 2025 and early 2026\. These will likely focus on systemic risk management, governance obligations, and ensuring content moderation systems are robust and effective.14  
* **Super-Complaints Regime:** A new "super-complaints" mechanism is expected to come into force by 31 December 2025\. This will allow designated consumer and child protection bodies to raise systemic issues about widespread harms on a service directly with Ofcom, compelling the regulator to respond.14

This phased implementation ensures that by 2026, a multi-layered regulatory structure will be in place, with universal duties for all services, heightened protections for children, and the most rigorous standards of accountability reserved for the platforms with the greatest reach and potential for harm.

| Phase | Key Duty/Requirement | In-Scope Services | Key Dates & Deadlines | Relevant Ofcom Guidance/Codes |
| :---- | :---- | :---- | :---- | :---- |
| **Phase 1** | **Illegal Content Duties** | All in-scope user-to-user and search services. | **Dec 2024:** Ofcom publishes Illegal Harms Codes of Practice & Risk Assessment Guidance. **16 Mar 2025:** Deadline to complete illegal content risk assessment. **17 Mar 2025:** Illegal content duties become enforceable by Ofcom. | Illegal Content Risk Assessment Guidance 12; Illegal Harms Codes of Practice 10 |
| **Phase 2** | **Child Safety Duties** | Services likely to be accessed by children. | **16 Apr 2025:** Deadline to complete Children's Access Assessment. **24 Jul 2025:** Deadline to complete Children's Risk Assessment. **25 Jul 2025:** Child safety duties become enforceable by Ofcom. | Children's Access Assessments Guidance 17; Children's Risk Assessment Guidance 12; Protection of Children Codes of Practice 1 |
| **Phase 2** | **Highly Effective Age Assurance (HEAA)** | Providers of pornographic content (Part 5 services); User-to-user services hosting pornographic content (Part 3). | **17 Jan 2025:** HEAA duties for pornographic publishers (Part 5\) come into force. **Jul 2025:** HEAA duties for user-to-user services (Part 3\) come into force. | Guidance on Highly Effective Age Assurance 12 |
| **Phase 3** | **Categorisation & Enhanced Duties** | Services meeting thresholds for Category 1, 2A, or 2B. | **Summer 2025:** Ofcom expected to publish register of categorised services. **Aug-Nov 2025:** Ofcom to issue draft and final transparency notices. **Late 2025 \- Early 2026:** Consultation on additional duties (e.g., systemic risk). | Draft Threshold Conditions Regulations 10; Draft Transparency Reporting Guidance 20 |
| **Phase 3** | **Super-Complaints Regime** | All in-scope services (relevant to complaints). | **31 Dec 2025:** Super-complaints regime expected to come into force. **Sep 2025:** Ofcom to consult on draft guidance for super-complainants. | Draft Guidance for Super-Complaints 17 |
| **Ongoing** | **Regulatory Fees Regime** | Providers exceeding the Qualifying Worldwide Revenue (QWR) threshold (expected Â£250 million). | **Oct 2025:** Fees regime comes into force; providers must notify Ofcom of liability. **Late 2025:** Government expected to lay secondary legislation for QWR threshold. | Consultation on Fees and Penalties 17; Consultation on QWR Guidance 14 |

## **Section 3: The Government's Strategic Priorities: Defining the Regulatory Mission**

While the Online Safety Act establishes the legal framework and Ofcom manages its technical implementation, the political direction and strategic emphasis of the regime are shaped by the government itself. This is achieved through a formal directive known as the Statement of Strategic Priorities (SSP) for Online Safety. Analysis of this document is critical to understanding how the Act is intended to be used for purposes beyond its headline goals of child safety and tackling illegal content, revealing a broader agenda that aligns the regulator's mission with the government's wider policy objectives.

### **The Statement of Strategic Priorities (SSP)**

The SSP is a document published by the Department for Science, Innovation and Technology (DSIT) that sets out the government's strategic priorities in relation to online safety matters.11 Under the Act, Ofcom is legally required to have regard to this statement when exercising its online safety functions, including its approach to enforcement and the development of its codes of practice.10 The final version of the SSP makes it clear that the government views the Act not merely as a tool for user protection but as a powerful instrument for advancing a spectrum of national policy goals.15

### **Beyond Core Harms: A Broader Mandate**

The SSP explicitly broadens the regulatory focus beyond the harms most commonly associated with the Act's public messaging. It directs Ofcom to prioritise a wider range of issues, effectively tasking the regulator with contributing to key government missions:

* **National Security:** The statement emphasizes the need for platforms to proactively identify and remove illegal content related to terrorism and "foreign interference".11 This positions the online safety regime as a component of the UK's national security apparatus, tasking a communications regulator with responsibilities that intersect with those of intelligence and law enforcement agencies.  
* **Violence Against Women and Girls:** The SSP directly links the Act's implementation to the government's specific political commitment to "halve violence against women and girls over the next decade".11 It states that the Act will be used to "tackle illegal and misogynistic content" as part of this mission, thereby using the regulatory framework to enforce a distinct social policy objective.15  
* **Economic Crime:** The document also highlights the importance of reducing the exploitation of online users by fraudsters, instructing Ofcom to prioritise work that will see a reduction in users' exposure to such content.11

This demonstrates a clear model of governance where a powerful and flexible regulatory framework, established under the banner of "online safety," is deliberately designed to be aimed at a range of evolving political priorities. The Act creates the enforcement mechanismâwith its threat of multi-billion-pound finesâand the SSP provides the political targeting instructions. This allows the executive to pursue policy goals in the digital sphere with the full force of the new regulator, bypassing the need to pass new primary legislation for each specific issue.

### **The Mandate on Mis- and Disinformation**

Perhaps the most significant and controversial aspect of the SSP is its focus on mis- and disinformation. While the Act itself largely avoided direct regulation of "legal but harmful" content for adults after earlier drafts were criticized for threatening free speech, the SSP reintroduces the issue as a strategic priority for the regulator.2  
The statement commits the government to promoting the public's "awareness of and resilience to misinformation and disinformation online".15 It instructs Ofcom to coordinate with technology companies, civil society, and education providers to deliver "interventions" to help the public identify and protect against what the government defines as "information threats." Furthermore, it tasks Ofcom with undertaking research and building evidence to keep pace with new and advancing issues, especially concerning the use of AI and the impact of geopolitical dynamics on the information environment.15  
This directive effectively deputises Ofcom to become a key actor in the government's strategy for managing the information landscape. While framed in the neutral language of media literacy and resilience, this mandate positions the regulator to influence how platforms handle content that, while legal, may be deemed undesirable or counter to the government's narrative. This moves the Act's function from the relatively clear-cut domain of illegal content into the highly contested and politically sensitive arena of information control.

## **Section 4: Critical Analysis: Unintended Consequences and Alternative Applications**

The Online Safety Act's journey into law has been shadowed by profound concerns from a broad coalition of civil liberties organisations, technology companies, and digital rights experts. While its stated aims are widely supported, the methods and mechanisms it employs have been heavily criticised for posing significant risks to fundamental rights, including freedom of expression, privacy, and digital security. This section critically analyses these concerns, exploring how the Act's implementation may lead to consequences that extend far beyond its primary safety objectives, potentially reshaping the internet for UK users and setting a dangerous global precedent.

### **The Chilling Effect: Freedom of Expression and the Risk of Over-Censorship**

A primary criticism of the Act is that it is likely to produce a significant "chilling effect" on free expression. This stems from the combination of vaguely defined categories of harm and the exceptionally severe penalties for non-compliance.6 Faced with the threat of fines amounting to up to 10% of their global annual revenue, platforms have a powerful financial incentive to adopt an overly cautious approach to content moderation, leading them to remove legitimate, lawful content to avoid any risk of regulatory action.3  
The Act's creation of a new "false communication offence," which can be triggered if a message is deemed to cause "psychological harm amounting to at least serious distress," has been identified as a direct threat to robust public discourse.7 Critics argue that such broad and subjective definitions of harm will inevitably lead to the suppression of controversial, satirical, or merely unpopular speech, as platforms become arbiters of emotional impact rather than legality.4 This has led to accusations that the legislation, under the guise of safety, is fundamentally designed to control the speed and intensity of online discourse and to manage dissent.4

### **The Privacy Paradox: Age Verification, Data Security, and Surveillance**

The Act's mandate for "Highly Effective Age Assurance" (HEAA) for any site hosting adult content fundamentally alters the user experience of the internet, moving away from a presumption of anonymity towards a model of mandatory identity verification.19 To comply, millions of UK users are now required to submit highly sensitive personal dataâsuch as passport details, driving licences, or biometric facial scansâto a host of third-party age verification providers in order to access a wide range of websites.3  
This creates a significant privacy paradox. In the name of protecting children, the Act has necessitated the creation of a vast new infrastructure for collecting and processing the personal data of the entire adult population. These centralised databases of sensitive information represent a high-value target for cybercriminals, raising profound concerns about the risk of catastrophic data breaches.21 The legislation, therefore, solves one problem by creating another, trading the risk of harm from content for the risk of harm from data insecurity and identity theft. Furthermore, this normalisation of age and identity checks establishes a framework that could easily be expanded for purposes of state surveillance, eroding the capacity for anonymous speech and association online.9

### **The Encryption Dilemma: Reconciling Security with Lawful Access**

One of the most fiercely debated aspects of the Act is its potential to undermine end-to-end encryption, a foundational technology for digital privacy and security. The Act grants Ofcom the power to issue notices requiring platforms to use "accredited technology" to identify and remove specified illegal content, such as CSEA and terrorist material, wherever it appears on their serviceâincluding within private, encrypted communications.4  
The only known technologies that could achieve this, such as "client-side scanning," involve scanning a user's messages on their own device before they are encrypted and sent. Security experts are nearly unanimous in their assessment that such systems would fatally compromise the integrity of end-to-end encryption, creating a "backdoor" that would be vulnerable to exploitation by criminals and hostile state actors, thereby weakening the security of all users.4 This has placed the UK government in direct conflict with major technology companies like Apple, WhatsApp, and Signal, which have publicly stated they would sooner withdraw their services from the UK market than compromise the security of their platforms.2 The government's pursuit of this power also stands in contrast to the advice of some senior national security professionals, who have argued that the societal benefits of strong, ubiquitous encryption far outweigh the risks.9

### **Mission Creep and the Regulation of Disinformation**

Building on the government's Strategic Priorities, the Act's framework is poised to become a powerful tool for state-sanctioned narrative control under the banner of combating disinformation. The SSP's directive for Ofcom to build "resilience to information threats" and to coordinate "interventions" in the information ecosystem provides a legal and regulatory basis for actions that could be perceived as policing the veracity of political and social content.15  
Critics point to the documented existence of UK government units targeting social media posts to "silence critics" as a worrying precedent for how these new, more powerful regulatory tools could be abused.22 The Act provides the architecture to formalise and scale such activities. While the initial focus is on illegal content, the framework is adaptable. The combination of Ofcom's broad mandate and the government's strategic direction creates a significant risk of "mission creep," where the regulator's focus shifts from protecting users from demonstrable, objective harms like CSEA to the far more subjective and politically charged task of managing legal but disfavoured information.4  
This entire legislative and regulatory structure has been described by its opponents as a "blueprint for repression".7 The Act's extraterritorial reach and its controversial requirements risk contributing to the fragmentation of the global internet. Faced with incompatible legal demands, global services may be forced to either block UK users, withdraw from the UK market entirely, or engineer a separate, less secure, and more heavily monitored version of their product specifically for the UK. This would create a "splinternet," where UK citizens have access to a different, and potentially inferior, set of online services than the rest of the world. Even more alarmingly, the Act's modelâcharacterised by general monitoring, mandatory age verification, potential backdoors into encryption, and state-directed content moderationâprovides a ready-made template that can be easily adopted and repurposed by authoritarian regimes worldwide to legitimise their own, more overt, systems of censorship and surveillance.

## **Section 5: Conclusion and Forward Outlook**

The United Kingdom's Online Safety Act 2023 stands as a landmark piece of legislation, unparalleled in its scope and ambition to re-engineer the relationship between online platforms, their users, and the state. It is an Act defined by a fundamental duality, embodying both a sincere and, in many respects, necessary attempt to address grave online harms while simultaneously creating a regulatory apparatus that poses profound and well-documented risks to the foundational principles of an open and free internet.

### **A Dual Mandate**

The analysis presented in this report demonstrates that the Act operates under a dual mandate. Its primary, public-facing mandate is one of protection. It rightly seeks to hold powerful technology companies accountable for the safety of their users, establishing clear legal duties to combat the proliferation of illegal content like child sexual abuse material and terrorism, and to create a safer environment for children online.1 The recent enforcement of child safety rules and the industry-wide changes in age verification are tangible outcomes of this protective agenda, responding to years of campaigning by victims' families and safety advocates.24  
However, operating in parallel is a second, more implicit mandate of control. The Act's architectureâwith its vast scope, extraterritorial reach, quasi-legislative regulator, and severe punitive powersâhas been purposefully designed as a flexible and powerful instrument of state policy. As directed by the government's Statement of Strategic Priorities, this instrument is being aimed at a wide array of political objectives, from national security and tackling misogyny to managing the highly contested space of mis- and disinformation.11 This has given rise to credible and serious concerns that the legislation will be used to chill free expression, normalise mass surveillance through identity verification, fatally weaken essential security technologies like end-to-end encryption, and ultimately exert a greater degree of state control over online discourse.7

### **The Road Ahead**

The implementation of the Online Safety Act is far from complete, and its final form will continue to be shaped by regulatory action, industry response, and legal challenges. The ongoing case brought by the Wikimedia Foundation against the categorization regulations is a significant early test of the Act's legal resilience and could set an important precedent for how its provisions are interpreted.7 The coming months and years will see critical developments, particularly as Ofcom consults on and finalises its enhanced duties for the largest platforms and as the standoff over end-to-end encryption intensifies. The balance that Ofcom strikes in its enforcementâparticularly its promise of a proportionate approach towards smaller, low-risk servicesâwill be crucial in determining whether the regime can achieve its safety goals without stifling innovation and community-led platforms.26

### **Global Precedent**

Ultimately, the UK Online Safety Act is a global test case. As nations around the world grapple with the same set of complex digital harms, the successes and failures of the UK's approach will be watched with intense interest and will inevitably influence the future of internet regulation worldwide.21 The central, unresolved question that the Act poses is whether it is possible to engineer a "safer" internet through comprehensive, top-down state regulation without fundamentally compromising the very characteristicsâprivacy, anonymity, and freedom of expressionâthat have made it a transformative force for communication and knowledge. The legacy of the Online Safety Act will be determined by its ability to navigate this inherent tension. Its failure to do so risks not only creating a more censored and fragmented internet for the people of the UK but also providing a dangerous blueprint for authoritarian control in the digital age.

#### **Works cited**

1. Online Safety Act \- GOV.UK, accessed September 29, 2025, [https://www.gov.uk/government/collections/online-safety-act](https://www.gov.uk/government/collections/online-safety-act)  
2. Online Safety Act 2023 \- Wikipedia, accessed September 29, 2025, [https://en.wikipedia.org/wiki/Online\_Safety\_Act\_2023](https://en.wikipedia.org/wiki/Online_Safety_Act_2023)  
3. UK's New Online Safety Act: What Consumers Need to Know | McAfee Blog, accessed September 29, 2025, [https://www.mcafee.com/blogs/internet-security/uks-new-online-safety-act-what-consumers-need-to-know/](https://www.mcafee.com/blogs/internet-security/uks-new-online-safety-act-what-consumers-need-to-know/)  
4. Online Safety Act: Privacy Threats and Free Speech Risks \- The Constitution Society, accessed September 29, 2025, [https://consoc.org.uk/the-online-safety-act-privacy-threats-and-free-speech-risks/](https://consoc.org.uk/the-online-safety-act-privacy-threats-and-free-speech-risks/)  
5. Online Safety Act: Everything You Need to Know \- Persona, accessed September 29, 2025, [https://withpersona.com/blog/online-safety-act](https://withpersona.com/blog/online-safety-act)  
6. LIBERTY'S BRIEFING ON THE ONLINE SAFETY BILL FOR SECOND READING IN THE HOUSE OF COMMONS, accessed September 29, 2025, [https://www.libertyhumanrights.org.uk/wp-content/uploads/2022/04/Libertys-second-reading-briefing-on-the-Online-Safety-Bill-for-the-House-of-Commons-April-2022.pdf](https://www.libertyhumanrights.org.uk/wp-content/uploads/2022/04/Libertys-second-reading-briefing-on-the-Online-Safety-Bill-for-the-House-of-Commons-April-2022.pdf)  
7. The UK Online Safety Bill: A Massive Threat to Online Privacy, Security, and Speech, accessed September 29, 2025, [https://www.eff.org/pages/uk-online-safety-bill-massive-threat-online-privacy-security-and-speech](https://www.eff.org/pages/uk-online-safety-bill-massive-threat-online-privacy-security-and-speech)  
8. The UK Online Safety Bill: who is in scope? \- Osborne Clarke, accessed September 29, 2025, [https://www.osborneclarke.com/insights/uk-online-safety-bill-who-scope](https://www.osborneclarke.com/insights/uk-online-safety-bill-who-scope)  
9. UK Online Safety Act \- Internet Society, accessed September 29, 2025, [https://www.internetsociety.org/resources/internet-fragmentation/uk-online-safety-act/](https://www.internetsociety.org/resources/internet-fragmentation/uk-online-safety-act/)  
10. Implementation of the Online Safety Act \- The House of Commons Library, accessed September 29, 2025, [https://commonslibrary.parliament.uk/research-briefings/cdp-2025-0043/](https://commonslibrary.parliament.uk/research-briefings/cdp-2025-0043/)  
11. Draft Statement of Strategic Priorities for Online Safety \- GOV.UK, accessed September 29, 2025, [https://assets.publishing.service.gov.uk/media/681c8b58275cb67b18d8709d/Draft\_Statement\_of\_Strategic\_Priorities\_for\_Online\_Safety.pdf](https://assets.publishing.service.gov.uk/media/681c8b58275cb67b18d8709d/Draft_Statement_of_Strategic_Priorities_for_Online_Safety.pdf)  
12. Online Safety Act: explainer \- GOV.UK, accessed September 29, 2025, [https://www.gov.uk/government/publications/online-safety-act-explainer/online-safety-act-explainer](https://www.gov.uk/government/publications/online-safety-act-explainer/online-safety-act-explainer)  
13. Online Safety Act compliance is coming \- Taylor Wessing, accessed September 29, 2025, [https://www.taylorwessing.com/en/insights-and-events/insights/2025/01/rd-online-safety-act-compliance-is-coming](https://www.taylorwessing.com/en/insights-and-events/insights/2025/01/rd-online-safety-act-compliance-is-coming)  
14. Online Safety Act Implementation: What's Changing and What's Next \- techUK, accessed September 29, 2025, [https://www.techuk.org/resource/online-safety-act-implementation-what-s-changing-and-what-s-next.html](https://www.techuk.org/resource/online-safety-act-implementation-what-s-changing-and-what-s-next.html)  
15. Final Statement of Strategic Priorities for Online Safety \- GOV.UK, accessed September 29, 2025, [https://www.gov.uk/government/publications/statement-of-strategic-priorities-for-online-safety/final-statement-of-strategic-priorities-for-online-safety](https://www.gov.uk/government/publications/statement-of-strategic-priorities-for-online-safety/final-statement-of-strategic-priorities-for-online-safety)  
16. Ofcom rolls out implementation phases for compliance with the Online Safety Act \- RPC, accessed September 29, 2025, [https://www.rpclegal.com/snapshots/technology-digital/winter-2024/ofcom-rolls-out-implementation-phases-for-compliance-with-the-online-safety-act/](https://www.rpclegal.com/snapshots/technology-digital/winter-2024/ofcom-rolls-out-implementation-phases-for-compliance-with-the-online-safety-act/)  
17. Important dates for Online Safety compliance \- Ofcom, accessed September 29, 2025, [https://www.ofcom.org.uk/online-safety/illegal-and-harmful-content/important-dates-for-online-safety-compliance](https://www.ofcom.org.uk/online-safety/illegal-and-harmful-content/important-dates-for-online-safety-compliance)  
18. UK Online Safety Act 2023 | Reed Smith LLP, accessed September 29, 2025, [https://www.reedsmith.com/en/topics/uk-online-safety-act-2023](https://www.reedsmith.com/en/topics/uk-online-safety-act-2023)  
19. No, the UK's Online Safety Act Doesn't Make Children Safer Online, accessed September 29, 2025, [https://www.eff.org/deeplinks/2025/08/no-uks-online-safety-act-doesnt-make-children-safer-online](https://www.eff.org/deeplinks/2025/08/no-uks-online-safety-act-doesnt-make-children-safer-online)  
20. Ofcom's approach to implementing the Online Safety Act, accessed September 29, 2025, [https://www.ofcom.org.uk/online-safety/illegal-and-harmful-content/roadmap-to-regulation](https://www.ofcom.org.uk/online-safety/illegal-and-harmful-content/roadmap-to-regulation)  
21. How the UK KILLED Privacy: The Online Safety Act Nightmare \- YouTube, accessed September 29, 2025, [https://www.youtube.com/watch?v=Vfve9jbVU48](https://www.youtube.com/watch?v=Vfve9jbVU48)  
22. How the UK KILLED Privacy: The Online Safety Act Nightmare \- YouTube, accessed September 29, 2025, [https://www.youtube.com/watch?v=Vfve9jbVU48\&vl=en](https://www.youtube.com/watch?v=Vfve9jbVU48&vl=en)  
23. The UK's Online Safety Act and protecting users \- Legal Help, accessed September 29, 2025, [https://support.google.com/legal-help-center/answer/15957560?hl=en](https://support.google.com/legal-help-center/answer/15957560?hl=en)  
24. Online safety \- Ofcom, accessed September 29, 2025, [https://www.ofcom.org.uk/online-safety](https://www.ofcom.org.uk/online-safety)  
25. Instagram still poses risk to children despite new safety tools, says Meta whistleblower, accessed September 29, 2025, [https://www.theguardian.com/technology/2025/sep/25/instagram-risk-children-safety-tools-meta](https://www.theguardian.com/technology/2025/sep/25/instagram-risk-children-safety-tools-meta)  
26. Repeal the Online Safety Act \- Petitions, accessed September 29, 2025, [https://petition.parliament.uk/petitions/722903](https://petition.parliament.uk/petitions/722903)